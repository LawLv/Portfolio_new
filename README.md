# Portfolio_new: æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨é‡‘èæŠ•èµ„ç»„åˆç®¡ç†ä¸­çš„åº”ç”¨  
*A Deep Reinforcement Learning Framework for Financial Portfolio Management*

æœ¬é¡¹ç›®åŸºäºè®ºæ–‡ [Jiang et al., 2017] æå‡ºçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå®ç°åœ¨åŠ å¯†è´§å¸å¸‚åœºä¸­è¿›è¡ŒæŠ•èµ„ç»„åˆè‡ªåŠ¨ç®¡ç†ã€‚é€šè¿‡ç¥ç»ç½‘ç»œç­–ç•¥å’Œå¤šç›®æ ‡ä¼˜åŒ–ï¼Œæ™ºèƒ½ä»£ç†åœ¨è€ƒè™‘é£é™©çš„åŒæ—¶è¿½æ±‚é«˜æ”¶ç›Šã€‚

---

## ğŸ“Œ é¡¹ç›®äº®ç‚¹ (Highlights)

- ğŸ“ˆ å®ç°å¤šç§åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æŠ•èµ„ç»„åˆç­–ç•¥  
  *Implemented CNN, RNN, and LSTM-based strategies for portfolio selection*

- ğŸ§  å¼•å…¥å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ ï¼Œä¼˜åŒ–é£é™©/æ”¶ç›Šå¹³è¡¡  
  *Introduced multi-objective reward functions to balance return and volatility*

- ğŸ” ä½¿ç”¨ NSGA-II é—ä¼ ç®—æ³•ä¼˜åŒ–ç­–ç•¥é€‰æ‹©  
  *Used NSGA-II to find Pareto-optimal strategies*

- ğŸ§ª å¤šè½®å›æµ‹æ˜¾è‘—ä¼˜äºåŸºå‡†ç­–ç•¥  
  *Achieved better performance than baseline methods (CRP, OLMAR)*

---

## é¡¹ç›®èƒŒæ™¯ (Background)

é‡‘èå¸‚åœºå…·æœ‰é«˜æ³¢åŠ¨æ€§å’Œä¸ç¡®å®šæ€§ï¼Œä¼ ç»Ÿç­–ç•¥éš¾ä»¥æ³›åŒ–ã€‚æœ¬é¡¹ç›®ç»“åˆå¼ºåŒ–å­¦ä¹ å’Œæ·±åº¦ç¥ç»ç½‘ç»œï¼Œåœ¨æ— éœ€é¢„è®¾å¸‚åœºæ¨¡å‹çš„å‰æä¸‹ï¼Œä»å†å²æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æŠ•èµ„ç­–ç•¥ã€‚

> *This project learns portfolio policies from market data using deep RL, without explicit financial models.*

---

## æ–¹æ³•ç®€ä»‹ (Methodology Overview)

### ç½‘ç»œç»“æ„ (Network Architecture)

- **EIIE**: æ¯ä¸ªèµ„äº§ç‹¬ç«‹è¯„ä¼°ï¼Œç½‘ç»œå…±äº«å‚æ•°  
  *Each asset is evaluated independently using a shared CNN/RNN/LSTM model*

- **PVM**: å­˜å‚¨å†å²æƒé‡å‘é‡  
  *Stores recent portfolio weights to preserve memory*

- **OSBL**: åœ¨çº¿éšæœºæ‰¹æ¬¡è®­ç»ƒ  
  *Online Stochastic Batch Learning for time-sensitive updates*

### å¥–åŠ±å‡½æ•° (Reward Function)

- è€ƒè™‘å¹³å‡æ”¶ç›Šä¸ä¸‹è¡Œé£é™©  
  *Reward = Î± Ã— Mean(Return) âˆ’ Î² Ã— Std(Drawdown)*  
  *Designed for multi-objective reinforcement learning*

### å¤šç›®æ ‡ä¼˜åŒ– (Multi-objective Optimization)

- ä½¿ç”¨ NSGA-II æœç´¢å¸•ç´¯æ‰˜æœ€ä¼˜ç­–ç•¥é›†  
  *Non-dominated Sorting Genetic Algorithm II (NSGA-II) used for optimization*

---

## ä½¿ç”¨è¯´æ˜ (Usage)

### 1. æ•°æ®å‡†å¤‡ (Data Preparation)

```bash
# é»˜è®¤åœ¨çº¿ä¸‹è½½ï¼Œä¹Ÿå¯è¯»å–æœ¬åœ°ç¼“å­˜æ•°æ®
# Config file: net_config.json
```

### 2. ç½‘ç»œç»“æ„é…ç½® (Configure Model)

ç¼–è¾‘ `net_config.json` é…ç½®ç½‘ç»œç±»å‹ã€ç‰¹å¾ç»´åº¦ã€çª—å£é•¿åº¦ç­‰ã€‚

```json
{
  "net_type": "cnn",
  "input_window": 50,
  "features": 3,
  "assets": 11
}
```

### 3. æ¨¡å‹è®­ç»ƒ (Train Model)

```bash
python create_train_dir.py
python train.py --config net_config.json
```

æ”¯æŒå¤šç§ç§å­å¹¶è¡Œè®­ç»ƒï¼Œç»“æœå†™å…¥ `train_summary.csv`ã€‚

### 4. å›æµ‹æµ‹è¯• (Backtest)

```bash
python backtest.py --model model_name --config net_config.json
```

æ”¯æŒç»“æœå¯è§†åŒ–ã€‚

---

## å®éªŒç»“æœ (Results)

| Algorithm | fAPV (Final Accumulated Portfolio Value) | SR (Sharpe Ratio) | MDD (Max Drawdown) |
|-----------|------------------------------------------|-------------------|---------------------|
| elite1    | 45.43                                    | 0.0880            | 0.2204              |
| elite2    | 44.98                                    | 0.0888            | 0.2274              |
| origin    | 44.36                                    | 0.0887            | 0.2217              |
| OLMAR     | 4.59                                     | 0.0366            | 0.6049              |
| CRP       | 1.84                                     | 0.0342            | 0.2336              |

> *Our methods outperform CRP and OLMAR in both return and risk-adjusted metrics.*

---

## é¡¹ç›®ç»“æ„ (Project Structure)

```
Portfolio_new/
â”œâ”€â”€ data/                  # æ•°æ®ä¸ç¼“å­˜
â”œâ”€â”€ models/                # ä¿å­˜è®­ç»ƒæ¨¡å‹
â”œâ”€â”€ configs/               # å‚æ•°é…ç½®
â”œâ”€â”€ train.py               # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ backtest.py            # å›æµ‹è„šæœ¬
â”œâ”€â”€ create_train_dir.py    # åˆå§‹åŒ–è®­ç»ƒç›®å½•
â”œâ”€â”€ utils.py               # è¾…åŠ©å‡½æ•°
â””â”€â”€ net_config.json        # ç½‘ç»œé…ç½®æ–‡ä»¶
```

---

## åç»­æ”¹è¿›æ–¹å‘ (Future Work)

- æ›¿æ¢å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚ DDPG, PPO ç­‰ï¼‰
- æ¼”åŒ–æœç´¢ reward å‡½æ•°å‚æ•° (Î±, Î², k)
- å¼•å…¥æ›´å¤šå¸‚åœºæ•°æ®ï¼ˆè‚¡ç¥¨ã€ETFï¼‰
- åŠ å¼ºè·¨å‘¨æœŸè¡¨ç°ï¼ˆé€‚åº”ç‰›å¸‚ä¸ç†Šå¸‚ï¼‰
- æ”¯æŒåœ¨çº¿éƒ¨ç½²ä¸å®æ—¶é¢„æµ‹

---

## å¼•ç”¨ä¸å‚è€ƒ (Reference)

> Jiang, Zhengyao, Dixing Xu, and Jinjun Liang.  
> *A deep reinforcement learning framework for the financial portfolio management problem.*  
> IEEE Transactions on Neural Networks and Learning Systems, 2017.

---

## é¡¹ç›®ä½œè€… (Authors)

é™ˆé©¿æ¥ã€æ¨å¯èŠ¸ã€æ½˜è…¾  
æŒ‡å¯¼è€å¸ˆï¼šæ¨é¹  
HKUST, March 2023
